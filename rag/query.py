"""
RAG Query Handler.
Handles queries against the knowledge base with context-aware responses.
"""

from typing import List, Dict, Optional

from utils.logging import logger
from config import RAG_TOP_K, API_PROVIDER

# Ð”Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚ Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð°
if API_PROVIDER == "yandex":
    from rag.index_simple import simple_index as knowledge_index
    from services.yandex_client import yandex_gpt_client as ai_client
else:
    from rag.index import vector_index as knowledge_index
    from services.openai_client import openai_client as ai_client


async def query_knowledge_base(
    query: str,
    conversation_history: Optional[List[Dict]] = None
) -> str:
    """
    Query the knowledge base and generate response.
    
    Args:
        query: User's query
        conversation_history: Previous conversation messages
    
    Returns:
        Generated response based on retrieved context
    """
    try:
        # Search for relevant documents
        logger.debug(f"Searching knowledge base for: {query} (provider: {API_PROVIDER})")
        
        if API_PROVIDER == "yandex":
            # Keyword-based search for Yandex
            relevant_chunks = knowledge_index.keyword_retrieve(query, top_k=RAG_TOP_K)
            
            if not relevant_chunks:
                logger.warning("No relevant documents found, using fallback")
                return await _fallback_response(query, conversation_history)
            
            context = "\n\n".join(relevant_chunks)
        else:
            # Vector-based search for OpenAI
            results = knowledge_index.similarity_search_with_score(query, k=RAG_TOP_K)
            
            if not results:
                logger.warning("No relevant documents found, using fallback")
                return await _fallback_response(query, conversation_history)
            
            # Prepare context from retrieved documents
            context = _prepare_context(results)
        
        # Generate response with context
        response = await _generate_rag_response(
            query=query,
            context=context,
            conversation_history=conversation_history
        )
        
        return response
        
    except Exception as e:
        logger.error(f"Error querying knowledge base: {e}")
        # Fallback to regular GPT response
        return await _fallback_response(query, conversation_history)


def _prepare_context(results: List[tuple]) -> str:
    """
    Prepare context from search results.
    
    Args:
        results: List of (document, score) tuples
    
    Returns:
        Formatted context string
    """
    context_parts = []
    
    for i, (doc, score) in enumerate(results, 1):
        source = doc.metadata.get('source', 'Unknown')
        content = doc.page_content.strip()
        
        context_parts.append(
            f"[Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº {i}: {source}]\n{content}\n"
        )
    
    return "\n".join(context_parts)


async def _generate_rag_response(
    query: str,
    context: str,
    conversation_history: Optional[List[Dict]] = None
) -> str:
    """
    Generate response using RAG context.
    
    Args:
        query: User's query
        context: Retrieved context from knowledge base
        conversation_history: Previous conversation
    
    Returns:
        Generated response
    """
    # Build prompt with context
    system_prompt = """Ð¢Ñ‹ - Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ð½Ñ‚-ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð¿Ð¾ Ð»ÑŽÐºÐ°Ð¼ Ð¸ Ð´Ð¾Ð¶Ð´ÐµÐ¿Ñ€Ð¸ÐµÐ¼Ð½Ð¸ÐºÐ°Ð¼ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸ "Ð›Ð˜Ð¢Ð›Ð˜Ð”Ð•Ð ".

Ð¢Ð’ÐžÐ¯ Ð ÐžÐ›Ð¬:
- ÐŸÐ¾Ð¼Ð¾Ð³Ð°ÐµÑˆÑŒ Ð¸Ð½Ð¶ÐµÐ½ÐµÑ€Ð°Ð¼, Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸ÐºÐ°Ð¼, Ð¿Ñ€Ð¾Ñ€Ð°Ð±Ð°Ð¼ Ð¸ ÑÐ½Ð°Ð±Ð¶ÐµÐ½Ñ†Ð°Ð¼ Ð¿Ð¾Ð´Ð±Ð¸Ñ€Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ†Ð¸ÑŽ
- Ð Ð°ÑÑˆÐ¸Ñ„Ñ€Ð¾Ð²Ñ‹Ð²Ð°ÐµÑˆÑŒ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¼Ð°Ñ€ÐºÐ¸Ñ€Ð¾Ð²ÐºÐ¸ Ñ‚Ð¸Ð¿Ð° Ð¢Ðœ(Ð”400)-2-7-9-60
- ÐŸÑ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÑˆÑŒ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸, Ð²ÐµÑÐ°, Ñ€Ð°Ð·Ð¼ÐµÑ€Ñ‹
- ÐžÐ±ÑŠÑÑÐ½ÑÐµÑˆÑŒ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð¸Ñ Ð¼ÐµÐ¶Ð´Ñƒ Ñ‚Ð¸Ð¿Ð°Ð¼Ð¸ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ†Ð¸Ð¸ (Ð¿Ð»Ð°Ð²Ð°ÑŽÑ‰Ð¸Ðµ/Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ðµ Ð»ÑŽÐºÐ¸, ÐºÐ»Ð°ÑÑÑ‹ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸)
- ÐŸÐ¾Ð¼Ð¾Ð³Ð°ÐµÑˆÑŒ Ñ Ð¿Ð¾Ð´Ð±Ð¾Ñ€Ð¾Ð¼ Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ñ… ÑƒÑÐ»Ð¾Ð²Ð¸Ð¹ ÑÐºÑÐ¿Ð»ÑƒÐ°Ñ‚Ð°Ñ†Ð¸Ð¸

Ð’ÐÐ–ÐÐ«Ð• ÐŸÐ ÐÐ’Ð˜Ð›Ð:
1. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¢ÐžÐ›Ð¬ÐšÐž Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³Ð° Ð½Ð¸Ð¶Ðµ
2. Ð”Ð»Ñ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸Ðº Ñ†Ð¸Ñ‚Ð¸Ñ€ÑƒÐ¹ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ð²ÐµÑ, Ñ€Ð°Ð·Ð¼ÐµÑ€Ñ‹, Ð¼Ð°Ñ€ÐºÐ¸Ñ€Ð¾Ð²ÐºÑƒ)
3. Ð•ÑÐ»Ð¸ Ð² ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³Ðµ Ð½ÐµÑ‚ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ - Ñ‡ÐµÑÑ‚Ð½Ð¾ ÑÐºÐ°Ð¶Ð¸ Ð¾Ð± ÑÑ‚Ð¾Ð¼
4. Ð’ÑÐµÐ³Ð´Ð° ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°Ð¹ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»/Ð¼Ð°Ñ€ÐºÐ¸Ñ€Ð¾Ð²ÐºÑƒ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ†Ð¸Ð¸
5. ÐžÐ±ÑŠÑÑÐ½ÑÐ¹ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ñ‹ Ð¿Ð¾Ð½ÑÑ‚Ð½Ñ‹Ð¼ ÑÐ·Ñ‹ÐºÐ¾Ð¼
6. Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€ÑƒÐ¹ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹: Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸, Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ, Ð¿Ñ€ÐµÐ¸Ð¼ÑƒÑ‰ÐµÑÑ‚Ð²Ð°

ÐšÐžÐÐ¢Ð•ÐšÐ¡Ð¢ Ð˜Ð— Ð¢Ð•Ð¥ÐÐ˜Ð§Ð•Ð¡ÐšÐžÐ“Ðž ÐšÐÐ¢ÐÐ›ÐžÐ“Ð:
{context}

Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÑÑ‚Ð¾Ñ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð´Ð»Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ Ð¸ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð°."""
    
    # Prepare messages
    messages = [
        {
            "role": "system",
            "content": system_prompt.format(context=context)
        }
    ]
    
    # Add conversation history if available
    if conversation_history:
        # Limit history to avoid token limits
        recent_history = conversation_history[-6:]  # Last 3 exchanges
        messages.extend(recent_history)
    
    # Add current query
    messages.append({
        "role": "user",
        "content": query
    })
    
    # Generate response
    response = await ai_client.generate_text_response(messages)
    
    return response


async def _fallback_response(
    query: str,
    conversation_history: Optional[List[Dict]] = None
) -> str:
    """
    Fallback to regular GPT response when RAG fails.
    
    Args:
        query: User's query
        conversation_history: Previous conversation
    
    Returns:
        Generated response
    """
    logger.info("Using fallback response (no RAG context)")
    
    system_message = {
        "role": "system",
        "content": """Ð¢Ñ‹ - Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ð½Ñ‚ Ð¿Ð¾ Ð»ÑŽÐºÐ°Ð¼ Ð¸ Ð´Ð¾Ð¶Ð´ÐµÐ¿Ñ€Ð¸ÐµÐ¼Ð½Ð¸ÐºÐ°Ð¼ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸ "Ð›Ð˜Ð¢Ð›Ð˜Ð”Ð•Ð ". 
        
Ð‘Ð°Ð·Ð° Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð½Ðµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ ÑÑ‚Ð¾Ð¼Ñƒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑƒ.
Ð•ÑÐ»Ð¸ ÑÑ‚Ð¾ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¾ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ†Ð¸Ð¸ - Ð¸Ð·Ð²Ð¸Ð½Ð¸ÑÑŒ Ð¸ Ð¿Ð¾Ð¿Ñ€Ð¾ÑÐ¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³.
Ð•ÑÐ»Ð¸ ÑÑ‚Ð¾ Ð¾Ð±Ñ‰Ð¸Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ - Ð¼Ð¾Ð¶ÐµÑˆÑŒ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ, Ð½Ð¾ Ð¿Ñ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ´Ð¸, Ñ‡Ñ‚Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚ Ð½Ðµ Ð¸Ð· ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³Ð°."""
    }
    
    messages = [system_message]
    
    if conversation_history:
        messages.extend(conversation_history[-6:])
    
    messages.append({
        "role": "user",
        "content": query
    })
    
    response = await ai_client.generate_text_response(messages)
    
    return f"âš ï¸ Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³ Ð½Ðµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ ÑÑ‚Ð¾Ð¼Ñƒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑƒ.\n\n{response}\n\nðŸ’¡ Ð£Ð±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ, Ñ‡Ñ‚Ð¾ Ñ„Ð°Ð¹Ð» Litlider_Katalog_VCHSHG_2025.pdf Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð² ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ."


async def add_document_to_knowledge_base(file_path: str) -> dict:
    """
    Add a document to the knowledge base.
    
    Args:
        file_path: Path to document file
    
    Returns:
        Dictionary with status and details
    """
    try:
        from pathlib import Path
        from rag.loader import document_loader
        
        # Load document
        file_path = Path(file_path)
        documents = document_loader.load_document(file_path)
        
        # Add to index
        # Note: This function currently only works with vector index
        # For Yandex, re-index the entire directory instead
        if API_PROVIDER == "yandex":
            logger.warning("For Yandex, please re-index the entire directory instead of adding single documents")
            return {
                "success": False,
                "error": "Use directory re-indexing for Yandex",
                "message": "Ð”Ð»Ñ Yandex Ð¿ÐµÑ€ÐµÐ¸Ð½Ð´ÐµÐºÑÐ¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð²ÑÑŽ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ"
            }
        
        knowledge_index.add_documents(documents)
        
        logger.info(f"Added {file_path.name} to knowledge base")
        
        return {
            "success": True,
            "file": file_path.name,
            "chunks": len(documents),
            "message": f"Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ {file_path.name} ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ ({len(documents)} Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð²)"
        }
        
    except Exception as e:
        logger.error(f"Error adding document to knowledge base: {e}")
        return {
            "success": False,
            "error": str(e),
            "message": f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°: {e}"
        }


def get_knowledge_base_stats() -> dict:
    """
    Get statistics about the knowledge base.
    
    Returns:
        Dictionary with statistics
    """
    return knowledge_index.get_stats()

